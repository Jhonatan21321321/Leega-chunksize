Relatório de Processamento de Grandes Volumes de Dados Metodologia

    Utilizei ferramentas especializadas em Python, incluindo a IDE PyCharm e o Jupyter Notebook.
    Implementei a técnica de "chunking" para processar grandes volumes de dados de forma eficiente.
    Realizei testes em um ambiente dedicado exclusivamente para esta finalidade.
    Utilizei o Power BI para visualização e análise dos resultados, garantindo precisão e confiabilidade.

Técnica de Chunking

    Criei um arquivo py "0 teste chunksize" para validar o desempenho.
    O chunking de 450000 linhas proporcionou a leitura completa em 10.07 segundos.
    Esta abordagem equilibra o uso eficiente de recursos, otimizando o tempo de execução e o consumo de memória.

Resultados dos Testes

    Otimização de Performance com Chunking
    Nome do arquivo: 0 teste chunksize.py
        Memória utilizada: 14.49 MiB
        Tempo de execução: 10.87 segundos
        Total de linhas processadas: 5000000

    Produto Mais Vendido (por quantidade e canal)
    Nome do arquivo: 1 teste.py
        Canal: Offline
        Produto: Cereal
        Unidades vendidas: 1044443977

    Região com Maior Volume de Vendas (em units)
        Nome do arquivo: 2 teste.py
        País: Libéria
        Região: África Subsaariana
        Unidades vendidas:
            Libéria: 136188169
            África Subsaariana: 448485592

    Média de Vendas Mensais por Produto
    Nome do arquivo: 3 teste.py
        Exemplo para o mês de 2020-12:
            Produto: Clothes
            Total de unidades vendidas: 434434
            Média de unidades vendidas: 14030.129032
